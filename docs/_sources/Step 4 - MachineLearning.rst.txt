Step 4 - MachineLearning
=======================================
 

Code execution 
------------------------

.. code-block:: console

   python MachineLearning.py (DirectoryName in which files will be stored)
   For example: python MachineLearning.py 1
    

Actual Code in MachineLearning
----------------------------------------


Helper Functions and Imports.

.. code-block:: console

   from __future__ import absolute_import, division, print_function
   import argparse 
   from tensorflow.keras import Model, layers
   import numpy as np
   
   import pandas as pd
   from sklearn.preprocessing import StandardScaler 
   from numpy import genfromtxt
   from sklearn import svm 
   
   import matplotlib.pyplot as plt
   from sklearn.preprocessing import StandardScaler
   
   from sklearn.metrics import accuracy_score
   from sklearn.metrics import confusion_matrix
   import warnings
   warnings.filterwarnings('ignore')
   
   from datetime import datetime
   from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
   from sklearn.metrics import roc_auc_score
   from sklearn.model_selection import StratifiedKFold
   scaler = StandardScaler()
   import tensorflow.keras
   from sklearn.metrics import precision_score, recall_score, accuracy_score 
   from tensorflow.keras.models import Sequential, load_model
   from tensorflow.keras.layers import Dense, Dropout, Activation,ActivityRegularization
   from tensorflow.keras.utils import to_categorical
   from sklearn import metrics
   import tensorflow as tf
   from tensorflow.keras.layers import LSTM, GRU,Bidirectional
   from sklearn import preprocessing 
   from tensorflow.keras.layers import Reshape 
   import seaborn as sn
   import matplotlib.pyplot as plt
   from pylab import rcParams
   import sys
   import os
   from sklearn import tree, ensemble
   from imblearn.under_sampling import RandomUnderSampler
   from imblearn.pipeline import make_pipeline
   from sklearn.metrics import roc_auc_score


   def metric(name,best_model,x_train,y_train,x_test,y_test):
   
      y_pred = best_model.predict(x_test)
      sn.set(font_scale=2)
      rcParams['figure.figsize'] = 7, 7
      confusion_matrix = pd.crosstab(y_test.argmax(axis=1), y_pred.argmax(axis=1), rownames=['Actual'], colnames=['Predicted'])
      sn.heatmap(confusion_matrix, annot=True)

      plt.savefig(args.path+os.sep+name+"Test.png")
      plt.clf()
      confusion_matrix = pd.crosstab(y_train.argmax(axis=1), best_model.predict(x_train).argmax(axis=1), rownames=['Actual'], colnames=['Predicted'])
      sn.heatmap(confusion_matrix, annot=True)

      plt.savefig(args.path+os.sep+name+"Train.png")
      plt.clf()


   def plotting(history):
      fig = plt.figure()
      history_dict = history.history
      print(history_dict.keys())
      plt.subplot(2,1,1)
      plt.plot(history_dict['accuracy'])
      plt.plot(history_dict['val_accuracy'])
      plt.title('model accuracy')
      plt.ylabel('accuracy')
      plt.xlabel('epoch')
      plt.legend(['Training Set', 'Validation Set'], loc='lower right')

      plt.subplot(2,1,2)


      plt.plot( history_dict['loss'])
      plt.plot( history_dict['val_loss'])
      plt.title('model loss')
      plt.ylabel('loss')
      plt.xlabel('epoch')
      plt.legend(['Training Set', 'Validation Set'], loc='upper right')

      plt.tight_layout()


Machine learning models. This code is designed so that it can be used for hyper-parameter optimization.

.. code-block:: console

   import pandas as pd 
   from os import makedirs

   
   def model1(name,activationfunction=["sigmoid"],dropout=[5],optimizer = ["Adam"],batch_size =[5],epochs=[30],l1=[0.01],l2=[0],validationsplit=[0.3]):
      for activation in activationfunction:
         for drop in dropout:
            for opt in optimizer:
               for b in batch_size:
                  for e in epochs:
                     for v in validationsplit:
                        model = Sequential()
                        model.add(Dense(100, input_shape=(x_train.shape[1],)))
                        model.add(Activation(activation))                            
                        model.add(Dropout(drop))
                        model.add(Dense(50))
                        model.add(Activation(activation))
                        model.add(Dropout(drop))
                        model.add(Dense(1))
                        model.add(Activation("sigmoid"))
                        model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)
                        history = model.fit(X_train, Y_train,batch_size=b, epochs=e,validation_split=v,verbose=0)
                        loss, acc = model.evaluate(X_test, Y_test)
                        Y_pred = model.predict(X_test)
                        
                        print(str(activation),str(drop),str(opt),str(b),str(e),str(v))
                        print("train AUC",roc_auc_score(Y_train, model.predict(X_train)))
                        print("test AUC",roc_auc_score(Y_test, model.predict(X_test)))
                        
                        # Return class prediction probabilities.
                        return np.amax(Y_pred,axis=1)
 

               
 




Training model for all p-value thresholds.

.. code-block:: console

   direc = sys.argv[1]  
   trainpath= "./"+direc+os.sep+"train/"
   testpath = "./"+direc+os.sep+"test/"
   allsnps = os.listdir("./"+direc)

   from xgboost import XGBClassifier
   for files in allsnps:
      #print(files)
      if "_snps" not in files and "pv_" in files:
         x_train = pd.read_csv("./"+direc+os.sep+files+os.sep+'ptrain.raw', sep="\s+")
         x_test = pd.read_csv("./"+direc+os.sep+files+os.sep+'ptest.raw', sep="\s+") 
         y_train  = pd.read_csv(trainpath+'YRI.pheno', sep="\s+") 
         y_test= pd.read_csv(testpath+'YRI.pheno', sep="\s+")
         x_train =x_train.iloc[:,6:].values
         x_test  =x_test.iloc[:,6:].values
         y_train =y_train.iloc[:,2:].values
         y_test =y_test.iloc[:,2:].values


         scaler = StandardScaler()
         std_scale = preprocessing.StandardScaler().fit(x_train)
         x_train = std_scale.transform(x_train)
         x_test = std_scale.transform(x_test)

      

         X_train = x_train
         X_test = x_test
         Y_train = y_train
         Y_test = y_test
 






         activationFunctions = ["relu"]
         ddropout = [0.2]
         optimizer = ["Adam"]
         batchsize = [50]
         epochsnumber = [100]
         validation = [0.3]
         data = model1("ANN1",activationFunctions,ddropout,optimizer,batchsize,epochsnumber,l1=[0],l2=[0],validationsplit=validation)
         x = pd.DataFrame()
         x['0'] = data
         x.to_csv(direc+os.sep+files+os.sep+"ML_probability",header=False, index=False)
 






  

   
   
 
   