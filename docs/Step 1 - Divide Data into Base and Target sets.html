
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Step 1 - Divide Data into Base and Target sets &#8212; Benchmarking Polygenic Risk Scores vs. Machine Learning 1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Step 2 - CalculatePRS" href="Step%202%20-%20CalculatePRS.html" />
    <link rel="prev" title="Step 0 - Generate Data" href="Step%200%20-%20Generate%20Data.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="step-1-divide-data-into-base-and-target-sets">
<h1>Step 1 - Divide Data into Base and Target sets<a class="headerlink" href="#step-1-divide-data-into-base-and-target-sets" title="Permalink to this headline">¶</a></h1>
<p>When calculating PRS, the GWAS summary statistic file is the Base file, a training set in Machine learning, whereas the Target file is a test set in machine learning.</p>
<div class="section" id="code-execution">
<h2>Code execution<a class="headerlink" href="#code-execution" title="Permalink to this headline">¶</a></h2>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">python dividedata.py (DirectoryName in which files will be stored)</span>
<span class="go">For example: python dividedata.py 1</span>
<span class="go">This function extracts the data from the previous step, divides it into training and test sets, and calculates the GWAS.</span>
</pre></div>
</div>
</div>
<div class="section" id="actual-code-in-dividedata-py">
<h2>Actual Code in dividedata.py<a class="headerlink" href="#actual-code-in-dividedata-py" title="Permalink to this headline">¶</a></h2>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span>import pandas as pd
import os
from sklearn.model_selection import train_test_split
import sys

#def reformat():
#  pass


#def subsubsection(pheno,direc,name):
#    case = pheno.loc[pheno[&quot;phenotype&quot;]==1]
#    control = pheno.loc[pheno[&quot;phenotype&quot;]==0]
#    case.to_csv(direc+os.sep+name+&quot;_case_id.txt&quot;, index=False, columns=[&#39;user_id&#39;],header=False)
#    control.to_csv(direc+os.sep+name+&quot;_control_id.txt&quot;, index=False, columns=[&#39;user_id&#39;],header=False)
#    pheno.to_csv(direc+os.sep+name+&quot;_id.txt&quot;, index=False, columns=[&#39;user_id&#39;],header=False)



def saveandformatsamplefile(top,bottom,direc):
   ### Split test cases/controls and train cases/controls
   ###make a phenotype file
   #bottom[&#39;ID_2&#39;] = &#39;sample_&#39; + bottom[&#39;ID_2&#39;].astype(str)
   #bottom[&#39;ID_1&#39;] = &#39;sample_&#39; + bottom[&#39;ID_1&#39;].astype(str)

   phenotype = pd.DataFrame()
   phenotype[&quot;user_id&quot;] = bottom[&quot;ID_1&quot;].values
   phenotype[&quot;phenotype&quot;] = bottom[&quot;pheno&quot;].values
   phenotype.to_csv(direc+os.sep+&quot;phenotype.csv&quot;,sep=&quot;\t&quot;,index=False)

   ###make covarite file
   cov  = pd.DataFrame()
   cov[&quot;FID&quot;] = bottom[&quot;ID_2&quot;].values
   cov[&quot;IID&quot;] = bottom[&quot;ID_1&quot;].values
   cov[&quot;Sex&quot;] = 1
   cov[&quot;cov1&quot;] = bottom[&quot;sharedConfounder1_bin1&quot;].values
   cov[&quot;cov2&quot;] = bottom[&quot;independentConfounder2_cat_norm1&quot;].values
   cov[&quot;cov3&quot;] = bottom[&quot;independentConfounder2_cat_norm2&quot;].values
   cov[&quot;cov4&quot;] = bottom[&quot;independentConfounder3_cat_unif1&quot;].values
   sampletop  =  top.copy()
   samplebottom = bottom.copy()

   samplebottom[&quot;pheno&quot;] = samplebottom[&quot;pheno&quot;].apply(pd.to_numeric)
   samplebottom.pheno[samplebottom[&#39;pheno&#39;]&lt;0]=0
   samplebottom.pheno[samplebottom[&#39;pheno&#39;]&gt;0]=1
   samplebottom[&quot;pheno&quot;] = pd.to_numeric(samplebottom[&quot;pheno&quot;],downcast=&#39;integer&#39;)
   sample = pd.concat([sampletop, samplebottom], axis=0)
   sample= sample.astype(str)


   if &quot;test&quot; in direc:
      subsubsection(phenotype,direc,&quot;test&quot;)
      sample.to_csv(direc+os.sep+&quot;test_snptest.sample&quot;,index=False,sep=&quot; &quot;)

   if &quot;train&quot; in direc:
      subsubsection(phenotype,direc,&quot;train&quot;)
      sample.to_csv(direc+os.sep+&quot;train_snptest.sample&quot;,index=False,sep=&quot; &quot;)





   # Modify the cases/controls information because plink considers 1 as a control and 2 as a case, whereas other tools consider 0 as control and 1 as a case.
   sample.pheno[sample[&#39;pheno&#39;]==&#39;1&#39;]=&#39;2&#39;
   sample.pheno[sample[&#39;pheno&#39;]==&#39;0&#39;]=&#39;1&#39;

   data = sample[[&quot;ID_1&quot;,&quot;ID_2&quot;,&quot;missing&quot;,&quot;pheno&quot;]]
   if &quot;test&quot; in direc:
      data.to_csv(direc+os.sep+&quot;test.sample&quot;,index=False,sep=&quot; &quot;)

   if &quot;train&quot; in direc:
      data.to_csv(direc+os.sep+&quot;train.sample&quot;,index=False,sep=&quot; &quot;)

   samplebottom.pheno[samplebottom[&#39;pheno&#39;]==1]=&#39;2&#39;
   samplebottom.pheno[samplebottom[&#39;pheno&#39;]==0]=&#39;1&#39;

   cov[&quot;cov5&quot;] = bottom[&quot;sharedConfounder4_norm1&quot;].values
   cov[&quot;cov6&quot;] = bottom[&quot;independentConfounder4_norm1&quot;].values
   cov.to_csv(direc+os.sep+&quot;YRI.covariate&quot;,index=False,sep=&quot;\t&quot;)
   ###PRS phenotype
   phenotype = pd.DataFrame()
   phenotype[&quot;FID&quot;] = bottom[&quot;ID_2&quot;].values
   phenotype[&quot;IID&quot;] = bottom[&quot;ID_1&quot;].values
   phenotype[&quot;phenotype&quot;] = bottom[&quot;pheno&quot;].values
   phenotype.to_csv(direc+os.sep+&quot;YRI.pheno&quot;,sep=&quot;\t&quot;,index=False)
   ###NewSample file
   sample = pd.concat([top, bottom], axis=0)
   sample = sample[[&#39;ID_1&#39;,&#39;ID_2&#39;,  &#39;missing&#39;,&#39;pheno&#39;]]
   sample.to_csv(direc+os.sep+&quot;YRIs.sample&quot;,index=False,sep=&quot; &quot;)
   return phenotype[&#39;FID&#39;].values



def splitsample(sample,direc):
   # Extract the first row because it does not contain the sample information.
   sampletop  = sample.head(1)
   samplebottom = sample.tail(len(sample)-1)

   #Modify the sample ID&#39;s.
   samplebottom[&#39;ID_1&#39;] = samplebottom[&#39;ID_1&#39;].astype(str)+str(&quot;_&quot;) + samplebottom[&#39;ID_1&#39;].astype(str)
   samplebottom[&#39;ID_2&#39;] = samplebottom[&#39;ID_2&#39;].astype(str)+str(&quot;_&quot;) + samplebottom[&#39;ID_2&#39;].astype(str)
   #samplebottom[&#39;ID_1&#39;] = samplebottom[&#39;ID_1&#39;].astype(str)
   #samplebottom[&#39;ID_2&#39;] = samplebottom[&#39;ID_2&#39;].astype(str)

   samplebottom[&quot;pheno&quot;] = samplebottom[&quot;pheno&quot;].apply(pd.to_numeric)

   #PhenotypeSimulator generates continuous phenotype, which we converted to binary phenotype by thresholding on 0.
   samplebottom[&quot;pheno&quot;].values[samplebottom[&quot;pheno&quot;] &lt; 0] = 0
   samplebottom[&quot;pheno&quot;].values[samplebottom[&quot;pheno&quot;] &gt; 0] = 1
   samplebottom[&quot;pheno&quot;] = pd.to_numeric(samplebottom[&quot;pheno&quot;],downcast=&#39;integer&#39;)

   # Spit the samples. The default is 75 percent training and 25 percent test sets.
   x_train, x_test, y_train, y_test = train_test_split(samplebottom, samplebottom[&quot;pheno&quot;].values)
   sampletop.iloc[0,9]=&quot;B&quot;

   trainsample = saveandformatsamplefile(sampletop, x_train,direc+os.sep+&quot;train&quot;)
   testsample = saveandformatsamplefile(sampletop, x_test,direc+os.sep+&quot;test&quot;)
   return trainsample,testsample

def commit(direc,name):
   sample  = pd.read_csv(direc+os.sep+name+&quot;.sample&quot;,sep=&quot; &quot;)
   samplebottom = sample.tail(len(sample)-1)
   fam = pd.read_csv(direc+os.sep+name+&quot;.fam&quot;,sep=&quot;\s+&quot;,header=None)
   fam[5] = samplebottom[&#39;pheno&#39;].values
   fam.to_csv(direc+os.sep+name+&quot;.fam&quot;,header=False,index=False, sep=&quot; &quot;)



# Directory name in which files will be stored.
# Create four directories to contain train, test, and intermediate files.

direc = sys.argv[1]
if not os.path.isdir(direc):
   os.mkdir(direc)
if not os.path.isdir(direc+os.sep+&quot;test&quot;):
   os.mkdir(direc+os.sep+&quot;test&quot;)
if not os.path.isdir(direc+os.sep+&quot;train&quot;):
   os.mkdir(direc+os.sep+&quot;train&quot;)
if not os.path.isdir(direc+os.sep+&quot;files&quot;):
   os.mkdir(direc+os.sep+&quot;files&quot;)

testdirec = direc+os.sep+&quot;test&quot;
traindirec = direc+os.sep+&quot;train&quot;
filesdirec = direc+os.sep+&quot;files&quot;

# Read the sample files, and ensure path is correct.
originalsamples = pd.read_csv(&quot;/l/proj/kuin0009/MuhammadMuneeb/mlvsprs/generatedata/CEU_merge/Ysim_snptest.sample&quot;,sep=&quot; &quot;)

# This function splits the samples into training and test sets.
train,test = splitsample(originalsamples,direc)

# Extract test samples using bcftools
os.system(&quot;bcftools view -S ./&quot;+testdirec+os.sep+&quot;test_id.txt /l/proj/kuin0009/MuhammadMuneeb/mlvsprs/generatedata/CEU_merge/X.vcf  &gt; ./&quot;+testdirec+os.sep+&quot;test.vcf&quot;)
os.system(&quot; ./plink --vcf ./&quot;+testdirec+os.sep+&quot;test.vcf --make-bed --out ./&quot;+testdirec+os.sep+&quot;test&quot;)
os.system(&quot;./plink --bfile  ./&quot;+testdirec+os.sep+&quot;test  --recode --tab --out ./&quot;+testdirec+os.sep+&quot;test&quot;)

# Extract training samples using bcftools
os.system(&quot;bcftools view -S ./&quot;+traindirec+os.sep+&quot;train_id.txt /l/proj/kuin0009/MuhammadMuneeb/mlvsprs/generatedata/CEU_merge/X.vcf  &gt; ./&quot;+traindirec+os.sep+&quot;train.vcf&quot;)
os.system(&quot;./plink --vcf ./&quot;+traindirec+os.sep+&quot;train.vcf --make-bed  --out ./&quot;+traindirec+os.sep+&quot;train&quot;)

# Modify the fam file.
commit(testdirec,&quot;test&quot;)
commit(traindirec,&quot;train&quot;)

os.system(&quot;./plink --bfile  ./&quot;+traindirec+os.sep+&quot;train  --recode --tab --out ./&quot;+traindirec+os.sep+&quot;train&quot;)




# Calculate GWAS using plink
os.system(&quot;./plink --bfile ./&quot;+traindirec+os.sep+&quot;train --allow-no-sex --fisher --out ./&quot;+traindirec+os.sep+&quot;train&quot;)

# Calculate GWAS using Snptest
os.system(&quot;./gtool -P --ped ./&quot;+traindirec+os.sep+&quot;train.ped --map ./&quot;+traindirec+os.sep+&quot;train.map --og ./&quot;+traindirec+os.sep+&quot;train.gen --os ./&quot;+traindirec+os.sep+&quot;train_fake.sample&quot;)
os.system(&quot;bcftools convert ./&quot;+traindirec+os.sep+&quot;train.vcf  -g  ./&quot;+traindirec+os.sep+&quot;trains&quot;)
os.system(&quot;./snptest -data ./&quot;+traindirec+os.sep+&quot;trains.gen.gz ./&quot;+traindirec+os.sep+&quot;train_snptest.sample -o ./&quot;+traindirec+os.sep+&quot;train.sum -frequentist 1 -method score -pheno pheno&quot;)



snpteststats = pd.read_csv(traindirec+os.sep+&quot;train.sum&quot;,sep=&quot; &quot;,low_memory=False,skiprows = 10)
snpteststats = snpteststats.head(len(snpteststats)-1)
plinkstats = pd.read_csv(traindirec+os.sep+&quot;train.assoc.fisher&quot;,sep=&quot;\s+&quot;,low_memory=False)
gwasstats = pd.DataFrame()



# Change the column names as required by lassosum and plink.
# Ensure chromosome number is correct. We changed it in the later step.

gwasstats[&#39;CHR&#39;] = [&#39;0&#39;]*len(plinkstats)
gwasstats[&#39;BP&#39;] = plinkstats[&#39;BP&#39;].values
gwasstats[&#39;SNP&#39;] = plinkstats[&#39;SNP&#39;].values
gwasstats[&#39;A1&#39;] = snpteststats[&#39;alleleA&#39;].values
gwasstats[&#39;A2&#39;] = snpteststats[&#39;alleleB&#39;].values
gwasstats[&#39;N&#39;] = snpteststats[&#39;all_total&#39;].values
gwasstats[&#39;SE&#39;] = snpteststats[&#39;frequentist_add_se_1&#39;].values
gwasstats[&#39;P&#39;] = plinkstats[&#39;P&#39;].values
gwasstats[&#39;OR&#39;] = plinkstats[&#39;OR&#39;].values

# In simulated data, the imputation score is 1 as we have not used any imputation technique.
gwasstats[&#39;INFO&#39;] = 1
gwasstats[&#39;MAF&#39;] = snpteststats[&#39;all_maf&#39;].values
#gwasstats.to_csv(traindirec+os.sep+&quot;Data.txt.gz&quot;,index=False, sep=&quot;\t&quot;,compression=&#39;gzip&#39;)
gwasstats.to_csv(traindirec+os.sep+&quot;Data.txt&quot;,index=False, sep=&quot;\t&quot;)
</pre></div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Benchmarking Polygenic Risk Scores vs. Machine Learning</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Step%200%20-%20Generate%20Data.html">Step 0 - Generate Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Step 1 - Divide Data into Base and Target sets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#code-execution">Code execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="#actual-code-in-dividedata-py">Actual Code in dividedata.py</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Step%202%20-%20CalculatePRS.html">Step 2 - CalculatePRS</a></li>
<li class="toctree-l1"><a class="reference internal" href="Step%202.0%20-%20QCTarget.R.html">Step 2.0 - QCTarget.R</a></li>
<li class="toctree-l1"><a class="reference internal" href="Step%203%20-%20Pvaluethreshold.html">Step 3 - Pvaluethreshold</a></li>
<li class="toctree-l1"><a class="reference internal" href="Step%204%20-%20MachineLearning.html">Step 4 - MachineLearning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Step%205%20-%20GetResults.html">Step 5 - GetResults</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="Step%200%20-%20Generate%20Data.html" title="previous chapter">Step 0 - Generate Data</a></li>
      <li>Next: <a href="Step%202%20-%20CalculatePRS.html" title="next chapter">Step 2 - CalculatePRS</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Muhammad Muneeb and Samuel F. Feng.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/Step 1 - Divide Data into Base and Target sets.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>